{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFECV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import randint, uniform\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чтение файла CSV\n",
    "df = pd.read_excel('data/radiomics.xlsx', dtype=str, sheet_name='T2')\n",
    "df.dropna(thresh=50, inplace=True)\n",
    "df.drop(['№а/к', 'ФИО', 'пол','Feature Name','PyRadiomics',  'Numpy', 'SimpleITK', 'PyWavelet', 'Python', 'Settings','EnabledImageTypes','Hash', 'Dimensionality', 'Spacing', 'Size','Mean', 'Minimum', 'Maximum' ,'Hash.1', 'Spacing.1', 'Size.1','BoundingBox','VoxelNum', 'VolumeNum', 'CenterOfMassIndex', 'CenterOfMass' ], axis=1, inplace=True)\n",
    "df.rename(columns={'Unnamed: 1':'Diagnosis'}, inplace=True)\n",
    "df['Diagnosis'] = df['Diagnosis'].str.replace(r'^C.*', '1', regex=True)\n",
    "df['Diagnosis'] = df['Diagnosis'].str.replace(r'^D.*', '0', regex=True)\n",
    "df['Diagnosis']= df['Diagnosis'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_excel('df_with_index.xlsx', index=True)  # Сохранение с индексами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выбираем все колонки, кроме Diagnosis\n",
    "cols_to_process = [col for col in df.columns if col != 'Diagnosis']\n",
    "\n",
    "# Замена точек на запятые во всех ячейках выбранных колонок\n",
    "df[cols_to_process] = df[cols_to_process].applymap(\n",
    "    lambda x: str(x).replace(',', '.') if pd.notna(x) else x\n",
    ")\n",
    "\n",
    "# Принудительное приведение к типу float\n",
    "df[cols_to_process] = df[cols_to_process].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка данных (замените на ваш путь к файлу)\n",
    "# df = pd.read_excel('your_file.xlsx')\n",
    "\n",
    "# Предполагаем, что df уже загружен\n",
    "print(\"Размер данных:\", df.shape)\n",
    "print(\"\\nПервые 5 строк:\")\n",
    "print(df.head())\n",
    "\n",
    "# Проверка на пропущенные значения\n",
    "print(\"\\nПропущенные значения:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Удаление строк с пропущенными значениями или заполнение средним\n",
    "df = df.dropna()  # или df.fillna(df.mean(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделение на признаки и целевую переменную\n",
    "X = df.drop('Diagnosis', axis=1)\n",
    "y = df['Diagnosis']\n",
    "\n",
    "# Проверка баланса классов\n",
    "print(\"\\nРаспределение классов:\")\n",
    "print(y.value_counts())\n",
    "print(\"Процент больных:\", (y.sum() / len(y)) * 100)\n",
    "\n",
    "# Разделение на тренировочную и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Масштабирование признаков\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Анализ важности признаков с помощью статистических методов\n",
    "def statistical_feature_analysis(X, y):\n",
    "    \"\"\"Статистический анализ важности признаков\"\"\"\n",
    "    selector = SelectKBest(score_func=f_classif, k='all')\n",
    "    selector.fit(X, y)\n",
    "    \n",
    "    # Создаем DataFrame с результатами\n",
    "    feature_scores = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'F_Score': selector.scores_,\n",
    "        'P_Value': selector.pvalues_\n",
    "    })\n",
    "    \n",
    "    feature_scores = feature_scores.sort_values('F_Score', ascending=False)\n",
    "    return feature_scores\n",
    "\n",
    "# 2. Функция для обучения и оценки моделей\n",
    "def evaluate_models(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Оценка различных классификаторов\"\"\"\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(random_state=42),\n",
    "        'Linear Discriminant Analysis': LinearDiscriminantAnalysis(),\n",
    "        'Support Vector Classifier': SVC(random_state=42, probability=True),\n",
    "        'Random Forest': RandomForestClassifier(random_state=42),\n",
    "        'Gaussian Naive Bayes': GaussianNB(),\n",
    "        'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=42)\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        # Обучение модели\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Предсказания\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "        \n",
    "        # Метрики\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "        \n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else None\n",
    "        \n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-Score': f1,\n",
    "            'ROC-AUC': roc_auc\n",
    "        })\n",
    "        \n",
    "        print(f\"{name}: Accuracy = {accuracy:.3f}, F1 = {f1:.3f}\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "# 3. Функция для подбора гиперпараметров\n",
    "def hyperparameter_tuning(X_train, y_train):\n",
    "    \"\"\"Подбор гиперпараметров для лучших моделей\"\"\"\n",
    "    \n",
    "    # Настройка Random Forest\n",
    "    rf_param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    }\n",
    "    \n",
    "    rf_search = RandomizedSearchCV(\n",
    "        RandomForestClassifier(random_state=42),\n",
    "        rf_param_grid,\n",
    "        n_iter=50,\n",
    "        cv=5,\n",
    "        scoring='f1',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Настройка SVC\n",
    "    svc_param_grid = {\n",
    "        'C': uniform(0.1, 10),\n",
    "        'gamma': uniform(0.01, 1),\n",
    "        'kernel': ['rbf', 'linear']\n",
    "    }\n",
    "    \n",
    "    svc_search = RandomizedSearchCV(\n",
    "        SVC(probability=True, random_state=42),\n",
    "        svc_param_grid,\n",
    "        n_iter=50,\n",
    "        cv=5,\n",
    "        scoring='f1',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    svc_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Настройка Logistic Regression\n",
    "    lr_param_grid = {\n",
    "        'C': uniform(0.1, 10),\n",
    "        'penalty': ['l2', 'none'],\n",
    "        'solver': ['lbfgs', 'newton-cg']\n",
    "    }\n",
    "    \n",
    "    lr_search = RandomizedSearchCV(\n",
    "        LogisticRegression(random_state=42),\n",
    "        lr_param_grid,\n",
    "        n_iter=30,\n",
    "        cv=5,\n",
    "        scoring='f1',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    lr_search.fit(X_train, y_train)\n",
    "    \n",
    "    return {\n",
    "        'Random Forest': rf_search.best_estimator_,\n",
    "        'SVC': svc_search.best_estimator_,\n",
    "        'Logistic Regression': lr_search.best_estimator_\n",
    "    }\n",
    "\n",
    "# 4. Анализ важности признаков из лучших моделей\n",
    "def analyze_feature_importance(models, feature_names):\n",
    "    \"\"\"Анализ важности признаков из обученных моделей\"\"\"\n",
    "    importance_results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            # Для моделей с feature_importances_\n",
    "            importances = model.feature_importances_\n",
    "            indices = np.argsort(importances)[::-1]\n",
    "            \n",
    "            importance_df = pd.DataFrame({\n",
    "                'Feature': feature_names[indices],\n",
    "                'Importance': importances[indices]\n",
    "            })\n",
    "            importance_results[name] = importance_df\n",
    "            \n",
    "        elif hasattr(model, 'coef_'):\n",
    "            # Для линейных моделей с коэффициентами\n",
    "            if len(model.coef_.shape) > 1:\n",
    "                coef = model.coef_[0]  # для многоклассовой классификации\n",
    "            else:\n",
    "                coef = model.coef_\n",
    "            \n",
    "            indices = np.argsort(np.abs(coef))[::-1]\n",
    "            \n",
    "            importance_df = pd.DataFrame({\n",
    "                'Feature': feature_names[indices],\n",
    "                'Coefficient': coef[indices],\n",
    "                'Absolute_Coefficient': np.abs(coef[indices])\n",
    "            })\n",
    "            importance_results[name] = importance_df\n",
    "    \n",
    "    return importance_results\n",
    "\n",
    "# Основной процесс анализа\n",
    "print(\"=\" * 50)\n",
    "print(\"СТАТИСТИЧЕСКИЙ АНАЛИЗ ПРИЗНАКОВ\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Статистический анализ\n",
    "stat_results = statistical_feature_analysis(X, y)\n",
    "print(stat_results.head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ОЦЕНКА БАЗОВЫХ МОДЕЛЕЙ\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Оценка базовых моделей\n",
    "base_results = evaluate_models(X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ПОДБОР ГИПЕРПАРАМЕТРОВ ДЛЯ ЛУЧШИХ МОДЕЛЕЙ\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Подбор гиперпараметров\n",
    "best_models = hyperparameter_tuning(X_train_scaled, y_train)\n",
    "\n",
    "# Оценка настроенных моделей\n",
    "tuned_results = []\n",
    "for name, model in best_models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    tuned_results.append({\n",
    "        'Model': f'Tuned {name}',\n",
    "        'Accuracy': accuracy,\n",
    "        'F1-Score': f1\n",
    "    })\n",
    "    \n",
    "    print(f\"Tuned {name}: Accuracy = {accuracy:.3f}, F1 = {f1:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"АНАЛИЗ ВАЖНОСТИ ПРИЗНАКОВ\")\n",
    "print(\"=\" * 50)\n",
    "# Анализ важности признаков\n",
    "feature_importance = analyze_feature_importance(best_models, X.columns.values)\n",
    "\n",
    "# Вывод наиболее важных признаков\n",
    "for model_name, importance_df in feature_importance.items():\n",
    "    print(f\"\\nТоп-10 признаков по модели {model_name}:\")\n",
    "    print(importance_df.head(10))\n",
    "\n",
    "# 5. Визуализация результатов\n",
    "def plot_results(stat_results, feature_importance):\n",
    "    \"\"\"Визуализация результатов анализа\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Статистическая значимость\n",
    "    plt.subplot(2, 2, 1)\n",
    "    top_stats = stat_results.head(10)\n",
    "    plt.barh(top_stats['Feature'], top_stats['F_Score'])\n",
    "    plt.xlabel('F-Score')\n",
    "    plt.title('Топ-10 признаков по F-Score')\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    # Важность признаков из Random Forest\n",
    "    plt.subplot(2, 2, 2)\n",
    "    if 'Random Forest' in feature_importance:\n",
    "        rf_importance = feature_importance['Random Forest'].head(10)\n",
    "        plt.barh(rf_importance['Feature'], rf_importance['Importance'])\n",
    "        plt.xlabel('Importance')\n",
    "        plt.title('Топ-10 признаков (Random Forest)')\n",
    "        plt.gca().invert_yaxis()\n",
    "    \n",
    "    # Коэффициенты Logistic Regression\n",
    "    plt.subplot(2, 2, 3)\n",
    "    if 'Logistic Regression' in feature_importance:\n",
    "        lr_importance = feature_importance['Logistic Regression'].head(10)\n",
    "        plt.barh(lr_importance['Feature'], lr_importance['Absolute_Coefficient'])\n",
    "        plt.xlabel('Absolute Coefficient')\n",
    "        plt.title('Топ-10 признаков (Logistic Regression)')\n",
    "        plt.gca().invert_yaxis()\n",
    "    \n",
    "    # Сравнение моделей\n",
    "    plt.subplot(2, 2, 4)\n",
    "    all_results = pd.concat([base_results, pd.DataFrame(tuned_results)], ignore_index=True)\n",
    "    models = all_results['Model']\n",
    "    f1_scores = all_results['F1-Score']\n",
    "    \n",
    "    plt.barh(models, f1_scores)\n",
    "    plt.xlabel('F1-Score')\n",
    "    plt.title('Сравнение производительности моделей')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Построение графиков\n",
    "plot_results(stat_results, feature_importance)\n",
    "\n",
    "# 6. Детальный анализ пороговых значений для лучших признаков\n",
    "def analyze_thresholds(df, top_features, n_bins=10):\n",
    "    \"\"\"Анализ пороговых значений для топ-признаков\"\"\"\n",
    "    threshold_analysis = {}\n",
    "    \n",
    "    for feature in top_features:\n",
    "        # Создание бинов\n",
    "        df['bin'] = pd.qcut(df[feature], n_bins, duplicates='drop')\n",
    "        \n",
    "        # Анализ заболеваемости по бинам\n",
    "        bin_analysis = df.groupby('bin').agg({\n",
    "            'Diagnosis': ['mean', 'count']\n",
    "        }).round(3)\n",
    "        \n",
    "        bin_analysis.columns = ['Disease_Rate', 'Count']\n",
    "        bin_analysis = bin_analysis.reset_index()\n",
    "        \n",
    "        threshold_analysis[feature] = bin_analysis\n",
    "    \n",
    "    return threshold_analysis\n",
    "\n",
    "# Анализ порогов для топ-5 признаков\n",
    "top_features = stat_results['Feature'].head(5).tolist()\n",
    "threshold_results = analyze_thresholds(df, top_features)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"АНАЛИЗ ПОРОГОВЫХ ЗНАЧЕНИЙ ДЛЯ ТОП-ПРИЗНАКОВ\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for feature, analysis in threshold_results.items():\n",
    "    print(f\"\\n{feature}:\")\n",
    "    print(analysis)\n",
    "\n",
    "# 7. Финальный отчет\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ФИНАЛЬНЫЙ ОТЧЕТ\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Определяем наиболее согласованные топ-признаки\n",
    "all_important_features = set()\n",
    "\n",
    "for importance_df in feature_importance.values():\n",
    "    top_features = importance_df.head(5)['Feature'].tolist()\n",
    "    all_important_features.update(top_features)\n",
    "\n",
    "# Добавляем статистически значимые признаки\n",
    "stat_top_features = stat_results.head(5)['Feature'].tolist()\n",
    "all_important_features.update(stat_top_features)\n",
    "\n",
    "print(\"Наиболее значимые признаки для диагностики:\")\n",
    "for i, feature in enumerate(all_important_features, 1):\n",
    "    print(f\"{i}. {feature}\")\n",
    "\n",
    "# Рекомендации по пороговым значениям\n",
    "print(\"\\nРекомендуется провести дополнительный анализ для определения оптимальных пороговых значений этих признаков.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
